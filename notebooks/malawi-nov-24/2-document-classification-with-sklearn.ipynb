{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Document Classification System\n",
    "The NumPy (Numerical Python) library used for working iwith arrays, and the Scikit-learn library is a python library built on NumPy, SciPy and matplotlib for data analytics and machine learning. The NLTK (Natural Language Toolkit) provides access to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring that you have the necessary libraries\n",
    "# !pip install nltk\n",
    "# !pip install numpy\n",
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Reuters-21578 dataset is one of the most widely used data collections for text categorization research. It is a collection of documents with news articles and the original corpus has 10,369 documents and a vocabulary of 29,930 word and has labeled categories such as \"earnings\", \"acquisitions\".. etc. You can read metadata about the dataset on [Hugging Face](https://huggingface.co/datasets/ucirvine/reuters21578)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     /Users/dunstanmatekenya/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download the dataset\n",
    "nltk.download('reuters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Reuters-21578 dataset\n",
    "documents = reuters.fileids()\n",
    "train_docs = list(filter(lambda doc: doc.startswith(\"train\"), documents))\n",
    "test_docs = list(filter(lambda doc: doc.startswith(\"test\"), documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prepare your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the data by extracting the raw text and category labels for both the training and testing documents. Assumption is that each document has only one category label, so we take only the first category label for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "train_data = [reuters.raw(doc_id) for doc_id in train_docs]\n",
    "train_labels = [reuters.categories(doc_id)[0] for doc_id in train_docs]\n",
    "test_data = [reuters.raw(doc_id) for doc_id in test_docs]\n",
    "test_labels = [reuters.categories(doc_id)[0] for doc_id in test_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question-How many different classes are in the training data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore some of the training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article content: COMPUTER TERMINAL SYSTEMS &lt;CPML> COMPLETES SALE\n",
      "  Computer Terminal Systems Inc said\n",
      "  it has completed the sale of 200,000 shares of its common\n",
      "  stock, and warrants to acquire an additional one mln shares, to\n",
      "  &lt;Sedio N.V.> of Lugano, Switzerland for 50,000 dlrs.\n",
      "      The company said the warrants are exercisable for five\n",
      "  years at a purchase price of .125 dlrs per share.\n",
      "      Computer Terminal said Sedio also has the right to buy\n",
      "  additional shares and increase its total holdings up to 40 pct\n",
      "  of the Computer Terminal's outstanding common stock under\n",
      "  certain circumstances involving change of control at the\n",
      "  company.\n",
      "      The company said if the conditions occur the warrants would\n",
      "  be exercisable at a price equal to 75 pct of its common stock's\n",
      "  market price at the time, not to exceed 1.50 dlrs per share.\n",
      "      Computer Terminal also said it sold the technolgy rights to\n",
      "  its Dot Matrix impact technology, including any future\n",
      "  improvements, to &lt;Woodco Inc> of Houston, Tex. for 200,000\n",
      "  dlrs. But, it said it would continue to be the exclusive\n",
      "  worldwide licensee of the technology for Woodco.\n",
      "      The company said the moves were part of its reorganization\n",
      "  plan and would help pay current operation costs and ensure\n",
      "  product delivery.\n",
      "      Computer Terminal makes computer generated labels, forms,\n",
      "  tags and ticket printers and terminals.\n",
      "  \n",
      "\n",
      " n\\, Label: acq\n"
     ]
    }
   ],
   "source": [
    "print(\"Article content: {} n\\, Label: {}\".format(train_data[1], train_labels[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Vectorizing the text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vectorize the text data using the TfidVectorizer from scikit-learn. TF-IDF is an abbreviation for Term Frequency Inverse Document Frequency. This is very common algorithm to transform text into a meaningful representation of numbers which is used to fit machine algorithm for prediction. \n",
    "- Its worth noting that nowadays, this vectorization approach is not commonly used. We will cover **word embeddings** tomorrow which is a better approach to represent words as numbers because **vector embeddings** can capture semantic meanings better.\n",
    "\n",
    "For the sklearn TF-IDF vectorizer, you can learn more about it [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the text data\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=1000)\n",
    "X_train = vectorizer.fit_transform(train_data)\n",
    "X_test = vectorizer.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: What role are the ```stop words``` playing in the code above? You might have learned this from Prof. Mohamad Ali already."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training a Linear Support Vector Machine (LinearSVC) classifier using the vectorized training data and corresponding label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearSVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the classifier\n",
    "classifier = LinearSVC()\n",
    "classifier.fit(X_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluate the classifier used and calculate the accuracy score as well as some other metrics (Precision, Recall and F-1 score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.876117919841007\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "            acq       0.95      0.96      0.96       719\n",
      "           alum       0.33      0.18      0.24        22\n",
      "         barley       1.00      0.71      0.83        14\n",
      "            bop       0.77      0.80      0.79        30\n",
      "        carcass       0.79      0.65      0.71        17\n",
      "     castor-oil       0.00      0.00      0.00         1\n",
      "          cocoa       0.94      1.00      0.97        17\n",
      "        coconut       0.00      0.00      0.00         2\n",
      "    coconut-oil       0.00      0.00      0.00         2\n",
      "         coffee       0.89      0.96      0.92        25\n",
      "         copper       0.93      0.93      0.93        15\n",
      "           corn       0.85      0.81      0.83        48\n",
      "         cotton       1.00      0.86      0.92        14\n",
      "            cpi       0.62      0.62      0.62        24\n",
      "            cpu       0.00      0.00      0.00         1\n",
      "          crude       0.79      0.93      0.86       182\n",
      "            dfl       0.00      0.00      0.00         1\n",
      "            dlr       0.70      0.72      0.71        43\n",
      "            dmk       0.00      0.00      0.00         1\n",
      "           earn       0.98      0.99      0.98      1083\n",
      "           fuel       1.00      0.22      0.36         9\n",
      "            gas       0.75      0.33      0.46         9\n",
      "            gnp       0.59      0.89      0.71        19\n",
      "           gold       0.96      0.96      0.96        26\n",
      "          grain       0.71      0.77      0.74        77\n",
      "      groundnut       0.00      0.00      0.00         3\n",
      "           heat       1.00      0.75      0.86         4\n",
      "            hog       1.00      0.50      0.67         4\n",
      "        housing       1.00      0.67      0.80         3\n",
      "         income       1.00      0.80      0.89         5\n",
      "    instal-debt       1.00      1.00      1.00         1\n",
      "       interest       0.78      0.76      0.77       124\n",
      "            ipi       1.00      1.00      1.00        11\n",
      "     iron-steel       0.69      0.64      0.67        14\n",
      "            jet       0.00      0.00      0.00         1\n",
      "           jobs       0.73      0.85      0.79        13\n",
      "       l-cattle       0.00      0.00      0.00         2\n",
      "           lead       0.83      0.42      0.56        12\n",
      "            lei       1.00      1.00      1.00         3\n",
      "      livestock       0.50      0.50      0.50         6\n",
      "         lumber       0.00      0.00      0.00         5\n",
      "      meal-feed       0.20      0.17      0.18         6\n",
      "       money-fx       0.65      0.65      0.65        96\n",
      "   money-supply       0.80      0.83      0.81        29\n",
      "        naphtha       0.00      0.00      0.00         1\n",
      "        nat-gas       0.64      0.54      0.58        13\n",
      "         nickel       0.00      0.00      0.00         1\n",
      "        oilseed       0.54      0.54      0.54        13\n",
      "         orange       0.75      0.33      0.46         9\n",
      "      palladium       0.00      0.00      0.00         1\n",
      "       palm-oil       0.67      1.00      0.80         4\n",
      "       pet-chem       1.00      0.50      0.67         6\n",
      "       platinum       0.00      0.00      0.00         3\n",
      "         potato       1.00      0.67      0.80         3\n",
      "        propane       0.00      0.00      0.00         2\n",
      "       rape-oil       0.00      0.00      0.00         1\n",
      "       reserves       1.00      0.64      0.78        14\n",
      "         retail       1.00      1.00      1.00         1\n",
      "           rice       0.00      0.00      0.00         1\n",
      "         rubber       0.69      1.00      0.82         9\n",
      "           ship       0.39      0.41      0.40        39\n",
      "         silver       0.00      0.00      0.00         0\n",
      "        soy-oil       0.00      0.00      0.00         2\n",
      "        soybean       0.00      0.00      0.00         2\n",
      "strategic-metal       0.00      0.00      0.00         6\n",
      "          sugar       0.71      0.96      0.81        25\n",
      "            tea       0.00      0.00      0.00         3\n",
      "            tin       0.71      0.50      0.59        10\n",
      "          trade       0.70      0.93      0.80        76\n",
      "        veg-oil       0.54      0.64      0.58        11\n",
      "            wpi       0.62      0.56      0.59         9\n",
      "            yen       0.00      0.00      0.00         6\n",
      "           zinc       0.00      0.00      0.00         5\n",
      "\n",
      "       accuracy                           0.88      3019\n",
      "      macro avg       0.53      0.48      0.49      3019\n",
      "   weighted avg       0.86      0.88      0.87      3019\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dunstanmatekenya/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/dunstanmatekenya/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/dunstanmatekenya/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/dunstanmatekenya/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/dunstanmatekenya/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/dunstanmatekenya/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the classifier\n",
    "y_pred = classifier.predict(X_test)\n",
    "accuracy = accuracy_score(test_labels, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(classification_report(test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Classify new documents (new BBC headlines) by vectorizing them using the same TfidfVectorizer and predicting their labels using the trained classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels: ['ship' 'ship' 'acq']\n"
     ]
    }
   ],
   "source": [
    "# Classify new documents (recent headlines obtained from BBC news regarding Tunisia)\n",
    "new_docs = [\n",
    "    \"Tunisia says 23 people missing in Mediterranean sea.\",\n",
    "    \"Tunisia officials arrested in dispute over flag display.\",\n",
    "    \"Tunisia lawyer arrested during live news broadcast.\"\n",
    "]\n",
    "new_docs_vectors = vectorizer.transform(new_docs)\n",
    "predicted_labels = classifier.predict(new_docs_vectors)\n",
    "print(\"Predicted labels:\", predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did this classifier fare? What can you do to improve the model? <br>\n",
    "Ans: Experimenting with different preprocessing techniques, feature extraction models and classification algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying with a different classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps 1 - 3 will be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Reuters-21578 dataset\n",
    "documents = reuters.fileids()\n",
    "train_docs = list(filter(lambda doc: doc.startswith(\"train\"), documents))\n",
    "test_docs = list(filter(lambda doc: doc.startswith(\"test\"), documents))\n",
    "\n",
    "# Prepare the data\n",
    "train_data = [reuters.raw(doc_id) for doc_id in train_docs]\n",
    "train_labels = [reuters.categories(doc_id)[0] for doc_id in train_docs]\n",
    "test_data = [reuters.raw(doc_id) for doc_id in test_docs]\n",
    "test_labels = [reuters.categories(doc_id)[0] for doc_id in test_docs]\n",
    "\n",
    "# Vectorize the text data\n",
    "vectorizer = CountVectorizer(stop_words=\"english\", max_features=1000)\n",
    "X_train = vectorizer.fit_transform(train_data)\n",
    "X_test = vectorizer.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Classifier (Multinomial Naive Bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the classifier\n",
    "y_pred = classifier.predict(X_test)\n",
    "accuracy = accuracy_score(test_labels, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(classification_report(test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify new documents (recent headlines obtained from BBC news regarding Tunisia)\n",
    "new_docs = [\n",
    "    \"Tunisia says 23 people missing in Mediterranean sea.\",\n",
    "    \"Tunisia officials arrested in dispute over flag display.\",\n",
    "    \"Tunisia lawyer arrested during live news broadcast.\"\n",
    "]\n",
    "new_docs_vectors = vectorizer.transform(new_docs)\n",
    "predicted_labels = classifier.predict(new_docs_vectors)\n",
    "print(\"Predicted labels:\", predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion: Compare the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of classifier depends on the specific characteristics of your dataset and the problem at hand. Multinomial Naive Bayes is known to work well with text data and can handle high-dimensional feature spaces efficiently. However, it assumes that the features are independent of each other, which may not always be the case in real-world scenarios.\n",
    "\n",
    "You can also experiment with different classifiers, such as Logistic Regression, Random Forest, or Gradient Boosting, and compare their performance to find the best fit for your dataset. You can also refine the model by trying different feature extraction techniques and hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are also other ways you can approach this, for example, Document Classification using BERT. Here is a notebook example on Kaggle that you can explore: https://www.kaggle.com/code/merishnasuwal/document-classification-using-bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT (Bidirectional Encoder Representations from Transformers) and other Transformer encoder architectures can also be used on a variety of tasks in NLP (natural language processing). They compute vector-space representations of natural language that are suitable for use in deep learning models. The BERT family of models uses the Transformer encoder architecture to process each token of input text in the full context of all tokens before and after. BERT models are usually pre-trained on a large corpus of text, then fine-tuned for specific tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
